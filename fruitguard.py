# -*- coding: utf-8 -*-
"""FruitGuard.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E4WCWeCT2kjUt_NZntwIeq5UaSMMXCf9
"""

from google.colab import drive
drive.mount('/content/drive')

import os

# Create a folder inside Drive for this project
project_path = "/content/drive/MyDrive/FruitGuard"
os.makedirs(project_path, exist_ok=True)

print("Project folder created at:", project_path)

!pip install kaggle

import shutil
import os

# Make .kaggle folder
!mkdir -p ~/.kaggle

# Remove existing kaggle.json if it exists
kaggle_json_path = "/root/.kaggle/kaggle.json"
if os.path.exists(kaggle_json_path):
    os.remove(kaggle_json_path)

# Copy uploaded kaggle.json to ~/.kaggle/
shutil.copy("kaggle.json", "/root/.kaggle/")

# Set proper permissions
!chmod 600 /root/.kaggle/kaggle.json

# Download PlantVillage dataset from Kaggle
!kaggle datasets download -d crowdflower/plant-disease

# Unzip into FruitGuard project folder
!unzip -q plant-disease.zip -d "/content/drive/MyDrive/FruitGuard/dataset"

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d emmarex/plantdisease

!unzip plantdisease.zip -d plantvillage

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_dir = "/content/plantvillage/PlantVillage"

# Normalize & split dataset (80% train, 10% validation, 10% test)
datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2) # Use 0.2 for combined val/test split initially

train_generator = datagen.flow_from_directory(
    train_dir,
    target_size=(128, 128),   # resizing
    batch_size=32,
    class_mode='categorical',
    subset='training',
    shuffle=True
)

# Create separate validation and test generators from the 20% split
val_test_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.5) # Split the 20% into 10% val, 10% test

val_generator = val_test_datagen.flow_from_directory(
    train_dir,
    target_size=(128, 128),
    batch_size=32,
    class_mode='categorical',
    subset='validation', # This will be the first 10% of the 20% split
    shuffle=True
)

test_generator = val_test_datagen.flow_from_directory(
    train_dir,
    target_size=(128, 128),
    batch_size=32,
    class_mode='categorical',
    subset='training', # This will be the remaining 10% of the 20% split
    shuffle=True
)

print("Classes:", train_generator.class_indices)
print("Number of classes:", len(train_generator.class_indices))

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

model = Sequential([
    # 1st Conv layer
    Conv2D(32, (3,3), activation='relu', input_shape=(128,128,3)),
    MaxPooling2D(pool_size=(2,2)),

    # 2nd Conv layer
    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(pool_size=(2,2)),

    # 3rd Conv layer
    Conv2D(128, (3,3), activation='relu'),
    MaxPooling2D(pool_size=(2,2)),

    # Flatten & Dense layers
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),   # avoid overfitting
    Dense(len(train_generator.class_indices), activation='softmax')  # Output layer
])

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

history = model.fit(
    train_generator,
    epochs=10,   # first run, small number of epochs
    validation_data=val_generator
)

import matplotlib.pyplot as plt

# Accuracy plot
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.legend()
plt.title("Model Accuracy")
plt.show()

# Loss plot
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.title("Model Loss")
plt.show()

# Save the entire model
model.save("plant_disease_model.h5")
print("âœ… Model saved as plant_disease_model.h5")

from tensorflow.keras.models import load_model

# Load full model
loaded_model = load_model("plant_disease_model.h5")
print("âœ… Model loaded successfully!")

# Evaluate on the test set
test_loss, test_accuracy = model.evaluate(test_generator, verbose=1)
print(f"âœ… Test Accuracy: {test_accuracy*100:.2f}%")
print(f"âœ… Test Loss: {test_loss:.4f}")

import numpy as np

# Predict on test data
Y_pred = model.predict(test_generator, verbose=1)
y_pred = np.argmax(Y_pred, axis=1)  # Get the predicted class index

from sklearn.metrics import classification_report

print("ðŸ“Š Classification Report:")
print(classification_report(test_generator.classes, y_pred, target_names=list(test_generator.class_indices.keys())))

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(test_generator.classes, y_pred)
plt.figure(figsize=(10,8))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=list(test_generator.class_indices.keys()),
            yticklabels=list(test_generator.class_indices.keys()))
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("ðŸŒ¿ Plant Disease Detection - Confusion Matrix")
plt.show()

import numpy as np
from tensorflow.keras.preprocessing import image

import os
import random

# Path to the root of the unzipped dataset
dataset_root = "/content/plantvillage/plantvillage/PlantVillage"

# Get a list of all subdirectories (each subdirectory is a class)
class_dirs = [os.path.join(dataset_root, d) for d in os.listdir(dataset_root) if os.path.isdir(os.path.join(dataset_root, d))]

# Choose a random class directory
random_class_dir = random.choice(class_dirs)

# Get a list of all image files in the chosen class directory
image_files = [os.path.join(random_class_dir, f) for f in os.listdir(random_class_dir) if f.endswith('.JPG') or f.endswith('.jpg')]

# Choose a random image file
img_path = random.choice(image_files)

print(f"Using sample image: {img_path}")

# Load image and resize to match training size (128x128 as per the training generator)
img = image.load_img(img_path, target_size=(128, 128))

# Convert image to array
img_array = image.img_to_array(img)

# Expand dimensions (add batch size of 1)
img_array = np.expand_dims(img_array, axis=0)

# Normalize like training set
img_array /= 255.0

import os

dataset_path = "/content/drive/MyDrive/FruitGuard/dataset"
print("Files in dataset folder:", os.listdir(dataset_path))

# Make prediction
prediction = model.predict(img_array)

# Get predicted class index
predicted_class_index = np.argmax(prediction)

# Get class labels from the training generator
class_labels = list(train_generator.class_indices.keys())

# Get the predicted class label
predicted_class_label = class_labels[predicted_class_index]

print(f"Predicted class index: {predicted_class_index}")
print(f"Predicted class label: {predicted_class_label}")

# Predict class
prediction = model.predict(img_array)
predicted_class = np.argmax(prediction, axis=1)[0]

# Map back to class label
class_labels = list(train_generator.class_indices.keys())
print("ðŸŒ¿ Predicted Disease:", class_labels[predicted_class])

import matplotlib.pyplot as plt

plt.imshow(image.load_img(img_path, target_size=(224,224)))
plt.axis("off")
plt.title(f"Prediction: {class_labels[predicted_class]}")
plt.show()

# Save full model (architecture + weights + optimizer state)
model.save("plant_disease_model.h5")
print("âœ… Model saved successfully!")

from tensorflow.keras.models import load_model

# Load model from saved file
model = load_model("plant_disease_model.h5")
print("âœ… Model loaded successfully!")

prediction = model.predict(img_array)
predicted_class = np.argmax(prediction, axis=1)[0]
print("ðŸŒ¿ Predicted Disease:", class_labels[predicted_class])

!pip install streamlit
!pip install streamlit-option-menu

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import numpy as np
# from tensorflow.keras.models import load_model
# from tensorflow.keras.preprocessing import image
# import cv2
# 
# # Load trained model
# model = load_model("plant_disease_model.h5")
# 
# # Your class labels (change as per your dataset)
# class_labels = ['Healthy', 'Powdery Mildew', 'Rust']
# 
# # App Title
# st.title("ðŸŒ¿ Plant Disease Detection")
# st.write("Upload a leaf image and Iâ€™ll predict if itâ€™s healthy or diseased!")
# 
# # Upload Image
# uploaded_file = st.file_uploader("Upload Leaf Image", type=["jpg", "jpeg", "png"])
# 
# if uploaded_file is not None:
#     # Read image
#     file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)
#     img = cv2.imdecode(file_bytes, 1)
#     st.image(img, caption="Uploaded Leaf", use_column_width=True)
# 
#     # Preprocess
#     img_resized = cv2.resize(img, (128, 128))
#     img_array = np.expand_dims(img_resized, axis=0) / 255.0
# 
#     # Prediction
#     prediction = model.predict(img_array)
#     predicted_class = np.argmax(prediction, axis=1)[0]
# 
#     # Show result
#     st.subheader("âœ… Prediction: " + class_labels[predicted_class])
# 
#

!python app.py

!streamlit run app.py

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import numpy as np
# from tensorflow.keras.models import load_model
# from tensorflow.keras.preprocessing import image
# import cv2
# 
# # Load trained model
# model = load_model("plant_disease_model.h5")
# 
# # Your class labels (change as per your dataset)
# class_labels = ['Pepper__bell___Bacterial_spot', 'Pepper__bell___healthy', 'Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy', 'Tomato_Bacterial_spot', 'Tomato_Early_blight', 'Tomato_Late_blight', 'Tomato_Leaf_Mold', 'Tomato_Septoria_leaf_spot', 'Tomato_Spider_mites_Two_spotted_spider_mite', 'Tomato__Target_Spot', 'Tomato__Tomato_YellowLeaf__Curl_Virus', 'Tomato__Tomato_mosaic_virus', 'Tomato_healthy']
# 
# 
# # App Title
# st.title("ðŸŒ¿ Plant Disease Detection")
# st.write("Upload a leaf image and Iâ€™ll predict if itâ€™s healthy or diseased!")
# 
# # Upload Image
# uploaded_file = st.file_uploader("Upload Leaf Image", type=["jpg", "jpeg", "png"])
# 
# if uploaded_file is not None:
#     # Read image
#     file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)
#     img = cv2.imdecode(file_bytes, 1)
#     st.image(img, caption="Uploaded Leaf", use_column_width=True)
# 
#     # Preprocess
#     img_resized = cv2.resize(img, (128, 128))
#     img_array = np.expand_dims(img_resized, axis=0) / 255.0
# 
#     # Prediction
#     prediction = model.predict(img_array)
#     predicted_class = np.argmax(prediction, axis=1)[0]
# 
#     # Show result
#     st.subheader("âœ… Prediction: " + class_labels[predicted_class])

!streamlit run app.py